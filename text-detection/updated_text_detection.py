# -*- coding: utf-8 -*-
"""updated_text_detection.ipynb
Generated from a Jupyter Notebook in Google Colab
Code created by Griffin Consigli | griffinconsigli.com
"""

pip install torch transformers

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

model_name = "gpt2"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

# Generating text with gpt2 to be used later.

def gen_text_with_gpt2(prompt):
    inputs = tokenizer.encode(prompt, return_tensors='pt')
    outputs = model.generate(inputs, max_length=200, do_sample=True, num_beams=5, no_repeat_ngram_size=2, early_stopping=True)
    return tokenizer.decode(outputs[0], skip_special_tokens=True)

sample_prompt = "Hello," # Feel free to change the sample_prompt
print(gen_text_with_gpt2(sample_prompt))

# A function to calculate and return the probability that text is AI-generated based on the likelihood that gpt2 is to choose a specific sequence of tokens:

def calculate_token_probabilities(text):
    tokens = tokenizer.tokenize(text)
    token_ids = tokenizer.convert_tokens_to_ids(tokens)
    probabilities = []
    context_ids = []
    with torch.no_grad():
      for i, token_id in enumerate(token_ids):
         if i > 0:
           context_ids = token_ids[:i]
         input_ids = torch.tensor(context_ids).unsqueeze(0)
         if input_ids.nelement() == 0:
             input_ids = torch.tensor([tokenizer.bos_token_id]).unsqueeze(0)
         outputs = model(input_ids)
         logits = outputs.logits
         predicted_token_logits = logits[:, -1, :]
         probs = torch.softmax(predicted_token_logits, dim=-1)
         token_probability = probs[0, token_id].item()
         probabilities.append({
             "token": tokens[i],
             "probability": token_probability
         })
    return probabilities

if __name__ == '__main__':
    test_text = "Hello, this is the first time I've been able to do this. It's been a long time coming, but it's finally here." # Replace with text generated by AI earlier or directly generate text with AI using gen_text_with_gpt2("prompt/context")
    probabilities = calculate_token_probabilities(test_text)
    probabilities_total = 0
    for item in probabilities:
        print(f"Token: '{item['token']}', Probability: {item['probability']:.6f}")
        probabilities_total = probabilities_total + item['probability']
    print("Average Probability (Mean): " + str(round(probabilities_total / len(probabilities), 6))) # Returns the likelihood the text is AI-generated
